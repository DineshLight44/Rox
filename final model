# AI-based Cough & Respiratory Sound Disease Classification System
# Complete Jupyter Notebook Implementation

"""
Author: AI Assistant
Date: September 2025
Description: Complete implementation of a CNN+BiLSTM model for respiratory sound classification
"""

# ============================================================================
# 1. IMPORTS AND SETUP
# ============================================================================

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
import librosa
import librosa.display
import soundfile as sf
import warnings
warnings.filterwarnings('ignore')

# Deep Learning
import tensorflow as tf
from tensorflow.keras.models import Model, Sequential, load_model
from tensorflow.keras.layers import (Conv2D, MaxPooling2D, LSTM, Bidirectional, 
                                    Dense, Dropout, BatchNormalization, 
                                    Input, Reshape, GlobalAveragePooling2D,
                                    TimeDistributed, Flatten)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.regularizers import l2

# Audio processing
import noisereduce as nr
from scipy import signal
import random

# Set random seeds for reproducibility
np.random.seed(42)
tf.random.set_seed(42)
random.seed(42)

print("All libraries imported successfully!")
print(f"TensorFlow version: {tf.__version__}")
print(f"GPU Available: {tf.config.list_physical_devices('GPU')}")

# ============================================================================
# 2. CONFIGURATION AND CONSTANTS
# ============================================================================

class Config:
    # Audio parameters
    SAMPLE_RATE = 22050
    DURATION = 4.0  # 4 seconds fixed length
    N_MFCC = 13
    N_MELS = 128
    HOP_LENGTH = 512
    N_FFT = 2048
    
    # Model parameters
    BATCH_SIZE = 32
    EPOCHS = 100
    LEARNING_RATE = 0.001
    VALIDATION_SPLIT = 0.2
    TEST_SPLIT = 0.1
    
    # Data paths (adjust these according to your dataset locations)
    COUGHVID_PATH = "/path/to/coughvid_v3"
    RESPIRATORY_PATH = "/path/to/respiratory_sound_database"
    
    # Output paths
    MODEL_PATH = "models/"
    PLOTS_PATH = "plots/"
    
    # Classes
    CLASSES = ['Healthy', 'COVID-19', 'Asthma', 'COPD', 'Pneumonia', 'Bronchiolitis']
    N_CLASSES = len(CLASSES)

# Create directories
os.makedirs(Config.MODEL_PATH, exist_ok=True)
os.makedirs(Config.PLOTS_PATH, exist_ok=True)

print(f"Configuration set. Target classes: {Config.CLASSES}")

# ============================================================================
# 3. DATA LOADING AND PREPROCESSING FUNCTIONS
# ============================================================================

class AudioPreprocessor:
    def __init__(self, config):
        self.config = config
        
    def load_audio_file(self, file_path):
        """Load audio file and handle various formats"""
        try:
            audio, sr = librosa.load(file_path, sr=self.config.SAMPLE_RATE)
            return audio, sr
        except Exception as e:
            print(f"Error loading {file_path}: {e}")
            return None, None
    
    def apply_noise_reduction(self, audio, sr):
        """Apply noise reduction to audio"""
        try:
            # Use noisereduce library
            reduced_noise = nr.reduce_noise(y=audio, sr=sr)
            return reduced_noise
        except:
            return audio
    
    def normalize_audio(self, audio):
        """Normalize audio amplitude"""
        if len(audio) == 0:
            return audio
        return librosa.util.normalize(audio)
    
    def pad_or_truncate(self, audio, target_length):
        """Pad or truncate audio to target length"""
        if len(audio) > target_length:
            return audio[:target_length]
        else:
            pad_width = target_length - len(audio)
            return np.pad(audio, (0, pad_width), mode='constant')
    
    def extract_mfcc_features(self, audio, sr):
        """Extract MFCC features from audio"""
        mfccs = librosa.feature.mfcc(
            y=audio, 
            sr=sr, 
            n_mfcc=self.config.N_MFCC,
            hop_length=self.config.HOP_LENGTH,
            n_fft=self.config.N_FFT
        )
        return mfccs
    
    def extract_mel_spectrogram(self, audio, sr):
        """Extract Mel-spectrogram features from audio"""
        mel_spec = librosa.feature.melspectrogram(
            y=audio,
            sr=sr,
            n_mels=self.config.N_MELS,
            hop_length=self.config.HOP_LENGTH,
            n_fft=self.config.N_FFT
        )
        # Convert to dB
        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
        return mel_spec_db
    
    def preprocess_audio(self, file_path, feature_type='mfcc'):
        """Complete preprocessing pipeline for a single audio file"""
        # Load audio
        audio, sr = self.load_audio_file(file_path)
        if audio is None:
            return None
        
        # Apply noise reduction
        audio = self.apply_noise_reduction(audio, sr)
        
        # Normalize
        audio = self.normalize_audio(audio)
        
        # Pad or truncate to fixed length
        target_length = int(self.config.DURATION * sr)
        audio = self.pad_or_truncate(audio, target_length)
        
        # Extract features
        if feature_type == 'mfcc':
            features = self.extract_mfcc_features(audio, sr)
        else:  # mel_spectrogram
            features = self.extract_mel_spectrogram(audio, sr)
        
        return features.T  # Transpose for time-first format

# ============================================================================
# 4. DATA AUGMENTATION FUNCTIONS
# ============================================================================

class AudioAugmentor:
    def __init__(self, config):
        self.config = config
    
    def time_stretch(self, audio, rate=1.0):
        """Apply time stretching to audio"""
        return librosa.effects.time_stretch(audio, rate=rate)
    
    def pitch_shift(self, audio, sr, n_steps=0):
        """Apply pitch shifting to audio"""
        return librosa.effects.pitch_shift(audio, sr=sr, n_steps=n_steps)
    
    def add_noise(self, audio, noise_factor=0.005):
        """Add random noise to audio"""
        noise = np.random.normal(0, 1, len(audio))
        return audio + noise_factor * noise
    
    def augment_features(self, features, augmentation_factor=2):
        """Apply augmentation to feature arrays"""
        augmented_features = []
        
        for _ in range(augmentation_factor):
            # Time masking
            masked_features = features.copy()
            mask_size = np.random.randint(1, min(10, features.shape[0]))
            mask_start = np.random.randint(0, features.shape[0] - mask_size)
            masked_features[mask_start:mask_start + mask_size, :] = 0
            augmented_features.append(masked_features)
            
            # Frequency masking
            masked_features = features.copy()
            mask_size = np.random.randint(1, min(5, features.shape[1]))
            mask_start = np.random.randint(0, features.shape[1] - mask_size)
            masked_features[:, mask_start:mask_start + mask_size] = 0
            augmented_features.append(masked_features)
        
        return augmented_features

# ============================================================================
# 5. DATASET LOADING FUNCTIONS
# ============================================================================

def load_coughvid_dataset(config, preprocessor):
    """Load and preprocess COUGHVID dataset"""
    print("Loading COUGHVID dataset...")
    
    # This is a template - adjust based on actual dataset structure
    coughvid_data = []
    labels = []
    
    # Assuming metadata CSV exists
    metadata_path = os.path.join(config.COUGHVID_PATH, "metadata.csv")
    
    if os.path.exists(metadata_path):
        df = pd.read_csv(metadata_path)
        
        for idx, row in df.iterrows():
            audio_path = os.path.join(config.COUGHVID_PATH, row['filename'])
            
            if os.path.exists(audio_path):
                features = preprocessor.preprocess_audio(audio_path, feature_type='mfcc')
                if features is not None:
                    coughvid_data.append(features)
                    # Map labels according to your dataset
                    labels.append(row.get('diagnosis', 'Healthy'))
            
            if idx % 100 == 0:
                print(f"Processed {idx} COUGHVID files...")
    
    return np.array(coughvid_data), np.array(labels)

def load_respiratory_dataset(config, preprocessor):
    """Load and preprocess Respiratory Sound Database"""
    print("Loading Respiratory Sound Database...")
    
    respiratory_data = []
    labels = []
    
    # This is a template - adjust based on actual dataset structure
    data_path = config.RESPIRATORY_PATH
    
    if os.path.exists(data_path):
        for root, dirs, files in os.walk(data_path):
            for file in files:
                if file.endswith(('.wav', '.mp3', '.flac')):
                    file_path = os.path.join(root, file)
                    features = preprocessor.preprocess_audio(file_path, feature_type='mfcc')
                    
                    if features is not None:
                        respiratory_data.append(features)
                        # Extract label from filename or directory structure
                        label = extract_label_from_path(file_path, file)
                        labels.append(label)
    
    return np.array(respiratory_data), np.array(labels)

def extract_label_from_path(file_path, filename):
    """Extract label from file path or filename"""
    # This function should be customized based on your dataset structure
    filename_lower = filename.lower()
    
    if 'covid' in filename_lower:
        return 'COVID-19'
    elif 'asthma' in filename_lower:
        return 'Asthma'
    elif 'copd' in filename_lower:
        return 'COPD'
    elif 'pneumonia' in filename_lower:
        return 'Pneumonia'
    elif 'bronchiolitis' in filename_lower:
        return 'Bronchiolitis'
    else:
        return 'Healthy'

# ============================================================================
# 6. MODEL ARCHITECTURE
# ============================================================================

def build_cnn_bilstm_model(input_shape, n_classes):
    """
    Build CNN + BiLSTM hybrid model
    """
    input_layer = Input(shape=input_shape)
    
    # Reshape for CNN (add channel dimension)
    x = Reshape((input_shape[0], input_shape[1], 1))(input_layer)
    
    # CNN Feature Extraction
    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)
    x = Dropout(0.25)(x)
    
    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)
    x = Dropout(0.25)(x)
    
    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)
    x = Dropout(0.25)(x)
    
    # Reshape for RNN
    # Get the shape after CNN layers
    cnn_output_shape = x.shape
    x = Reshape((cnn_output_shape[1], cnn_output_shape[2] * cnn_output_shape[3]))(x)
    
    # BiLSTM layers for temporal modeling
    x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))(x)
    x = Bidirectional(LSTM(64, dropout=0.3, recurrent_dropout=0.3))(x)
    
    # Dense layers for classification
    x = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(x)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    
    x = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(x)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)
    
    # Output layer
    output = Dense(n_classes, activation='softmax', name='predictions')(x)
    
    model = Model(inputs=input_layer, outputs=output)
    
    return model

def build_cnn_model(input_shape, n_classes):
    """
    Build pure CNN model for comparison
    """
    model = Sequential([
        Input(shape=(*input_shape, 1)),
        
        Conv2D(32, (3, 3), activation='relu', padding='same'),
        BatchNormalization(),
        MaxPooling2D((2, 2)),
        Dropout(0.25),
        
        Conv2D(64, (3, 3), activation='relu', padding='same'),
        BatchNormalization(),
        MaxPooling2D((2, 2)),
        Dropout(0.25),
        
        Conv2D(128, (3, 3), activation='relu', padding='same'),
        BatchNormalization(),
        MaxPooling2D((2, 2)),
        Dropout(0.25),
        
        Conv2D(256, (3, 3), activation='relu', padding='same'),
        BatchNormalization(),
        GlobalAveragePooling2D(),
        Dropout(0.5),
        
        Dense(256, activation='relu', kernel_regularizer=l2(0.001)),
        BatchNormalization(),
        Dropout(0.3),
        
        Dense(n_classes, activation='softmax')
    ])
    
    return model

# ============================================================================
# 7. TRAINING FUNCTIONS
# ============================================================================

def train_model(model, X_train, y_train, X_val, y_val, config):
    """Train the model with callbacks"""
    
    # Compile model
    model.compile(
        optimizer=Adam(learning_rate=config.LEARNING_RATE),
        loss='categorical_crossentropy',
        metrics=['accuracy', 'precision', 'recall']
    )
    
    # Callbacks
    callbacks = [
        EarlyStopping(
            monitor='val_accuracy',
            patience=15,
            restore_best_weights=True,
            verbose=1
        ),
        ModelCheckpoint(
            filepath=os.path.join(config.MODEL_PATH, 'best_model.h5'),
            monitor='val_accuracy',
            save_best_only=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=10,
            min_lr=1e-7,
            verbose=1
        )
    ]
    
    print("Starting model training...")
    history = model.fit(
        X_train, y_train,
        batch_size=config.BATCH_SIZE,
        epochs=config.EPOCHS,
        validation_data=(X_val, y_val),
        callbacks=callbacks,
        verbose=1
    )
    
    return history

# ============================================================================
# 8. EVALUATION FUNCTIONS
# ============================================================================

def plot_training_history(history, config):
    """Plot training history"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # Accuracy
    axes[0, 0].plot(history.history['accuracy'], label='Training Accuracy')
    axes[0, 0].plot(history.history['val_accuracy'], label='Validation Accuracy')
    axes[0, 0].set_title('Model Accuracy')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Accuracy')
    axes[0, 0].legend()
    axes[0, 0].grid(True)
    
    # Loss
    axes[0, 1].plot(history.history['loss'], label='Training Loss')
    axes[0, 1].plot(history.history['val_loss'], label='Validation Loss')
    axes[0, 1].set_title('Model Loss')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Loss')
    axes[0, 1].legend()
    axes[0, 1].grid(True)
    
    # Precision
    if 'precision' in history.history:
        axes[1, 0].plot(history.history['precision'], label='Training Precision')
        axes[1, 0].plot(history.history['val_precision'], label='Validation Precision')
        axes[1, 0].set_title('Model Precision')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('Precision')
        axes[1, 0].legend()
        axes[1, 0].grid(True)
    
    # Recall
    if 'recall' in history.history:
        axes[1, 1].plot(history.history['recall'], label='Training Recall')
        axes[1, 1].plot(history.history['val_recall'], label='Validation Recall')
        axes[1, 1].set_title('Model Recall')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('Recall')
        axes[1, 1].legend()
        axes[1, 1].grid(True)
    
    plt.tight_layout()
    plt.savefig(os.path.join(config.PLOTS_PATH, 'training_history.png'), dpi=300, bbox_inches='tight')
    plt.show()

def plot_confusion_matrix(y_true, y_pred, class_names, config):
    """Plot confusion matrix"""
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names,
                yticklabels=class_names)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.tight_layout()
    plt.savefig(os.path.join(config.PLOTS_PATH, 'confusion_matrix.png'), dpi=300, bbox_inches='tight')
    plt.show()

def plot_roc_curves(y_true, y_pred_proba, class_names, config):
    """Plot ROC curves for multi-class classification"""
    from sklearn.metrics import roc_curve, auc
    from sklearn.preprocessing import label_binarize
    
    # Binarize the output
    y_true_bin = label_binarize(y_true, classes=range(len(class_names)))
    n_classes = len(class_names)
    
    plt.figure(figsize=(12, 8))
    colors = plt.cm.Set3(np.linspace(0, 1, n_classes))
    
    for i, color in zip(range(n_classes), colors):
        fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_pred_proba[:, i])
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, color=color, lw=2,
                label=f'{class_names[i]} (AUC = {roc_auc:.2f})')
    
    plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Multi-class ROC Curves')
    plt.legend(loc="lower right")
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(os.path.join(config.PLOTS_PATH, 'roc_curves.png'), dpi=300, bbox_inches='tight')
    plt.show()

def evaluate_model(model, X_test, y_test, class_names, config):
    """Complete model evaluation"""
    print("Evaluating model...")
    
    # Predictions
    y_pred_proba = model.predict(X_test, batch_size=config.BATCH_SIZE)
    y_pred = np.argmax(y_pred_proba, axis=1)
    y_true = np.argmax(y_test, axis=1)
    
    # Classification report
    report = classification_report(y_true, y_pred, 
                                 target_names=class_names,
                                 digits=4)
    print("Classification Report:")
    print(report)
    
    # Save report
    with open(os.path.join(config.MODEL_PATH, 'classification_report.txt'), 'w') as f:
        f.write(report)
    
    # Plot confusion matrix
    plot_confusion_matrix(y_true, y_pred, class_names, config)
    
    # Plot ROC curves
    plot_roc_curves(y_true, y_pred_proba, class_names, config)
    
    return y_pred, y_pred_proba

# ============================================================================
# 9. INFERENCE FUNCTIONS
# ============================================================================

class CoughClassifier:
    def __init__(self, model_path, config):
        self.model = load_model(model_path)
        self.config = config
        self.preprocessor = AudioPreprocessor(config)
        self.label_encoder = LabelEncoder()
        self.label_encoder.fit(config.CLASSES)
        
    def predict_single_file(self, audio_path):
        """Predict class for a single audio file"""
        try:
            # Preprocess audio
            features = self.preprocessor.preprocess_audio(audio_path, feature_type='mfcc')
            
            if features is None:
                return None, None
            
            # Reshape for model input
            features = np.expand_dims(features, axis=0)
            
            # Predict
            prediction_proba = self.model.predict(features, verbose=0)[0]
            predicted_class_idx = np.argmax(prediction_proba)
            predicted_class = self.config.CLASSES[predicted_class_idx]
            confidence = prediction_proba[predicted_class_idx]
            
            return predicted_class, confidence, prediction_proba
        
        except Exception as e:
            print(f"Error during prediction: {e}")
            return None, None, None
    
    def predict_batch(self, audio_paths):
        """Predict classes for multiple audio files"""
        results = []
        
        for path in audio_paths:
            result = self.predict_single_file(path)
            results.append({
                'file_path': path,
                'predicted_class': result[0],
                'confidence': result[1],
                'probabilities': result[2]
            })
        
        return results

# ============================================================================
# 10. MAIN EXECUTION PIPELINE
# ============================================================================

def main():
    """Main execution pipeline"""
    
    # Initialize configuration and preprocessor
    config = Config()
    preprocessor = AudioPreprocessor(config)
    augmentor = AudioAugmentor(config)
    
    print("=== Cough & Respiratory Sound Disease Classification System ===")
    print(f"Target classes: {config.CLASSES}")
    
    # Note: This is a template for data loading
    # You'll need to adjust the data loading functions based on your actual dataset structure
    
    # For demonstration, let's create some dummy data
    print("\n=== Creating dummy data for demonstration ===")
    print("Note: Replace this section with actual data loading from your datasets")
    
    # Create dummy data (replace with actual data loading)
    n_samples = 1000
    time_steps = int(config.DURATION * config.SAMPLE_RATE / config.HOP_LENGTH) + 1
    n_features = config.N_MFCC
    
    X_dummy = np.random.randn(n_samples, time_steps, n_features)
    y_dummy = np.random.randint(0, config.N_CLASSES, n_samples)
    
    print(f"Dummy data shape: {X_dummy.shape}")
    print(f"Dummy labels shape: {y_dummy.shape}")
    
    # Convert labels to categorical
    y_categorical = to_categorical(y_dummy, num_classes=config.N_CLASSES)
    
    # Split data
    X_train_val, X_test, y_train_val, y_test = train_test_split(
        X_dummy, y_categorical, 
        test_size=config.TEST_SPLIT, 
        random_state=42, 
        stratify=y_dummy
    )
    
    X_train, X_val, y_train, y_val = train_test_split(
        X_train_val, y_train_val,
        test_size=config.VALIDATION_SPLIT,
        random_state=42
    )
    
    print(f"\nData splits:")
    print(f"Training: {X_train.shape[0]} samples")
    print(f"Validation: {X_val.shape[0]} samples")
    print(f"Test: {X_test.shape[0]} samples")
    
    # Build model
    print("\n=== Building CNN + BiLSTM Model ===")
    input_shape = (X_train.shape[1], X_train.shape[2])  # (time_steps, features)
    model = build_cnn_bilstm_model(input_shape, config.N_CLASSES)
    
    # Print model summary
    print("\nModel Architecture:")
    model.summary()
    
    # Train model
    print("\n=== Training Model ===")
    history = train_model(model, X_train, y_train, X_val, y_val, config)
    
    # Plot training history
    plot_training_history(history, config)
    
    # Evaluate model
    print("\n=== Evaluating Model ===")
    y_pred, y_pred_proba = evaluate_model(model, X_test, y_test, config.CLASSES, config)
    
    # Save final model
    model.save(os.path.join(config.MODEL_PATH, 'final_cough_classifier.h5'))
    print(f"Model saved to {config.MODEL_PATH}")
    
    # Demonstrate inference
    print("\n=== Inference Demo ===")
    classifier = CoughClassifier(
        os.path.join(config.MODEL_PATH, 'final_cough_classifier.h5'), 
        config
    )
    
    print("Inference system ready!")
    print("Use classifier.predict_single_file('path_to_audio.wav') to classify new recordings")
    
    return model, history, classifier

# ============================================================================
# 11. ADDITIONAL UTILITY FUNCTIONS
# ============================================================================

def visualize_audio_features(audio_path, preprocessor, config):
    """Visualize audio waveform and extracted features"""
    
    # Load and preprocess audio
    audio, sr = preprocessor.load_audio_file(audio_path)
    if audio is None:
        print("Could not load audio file")
        return
    
    # Create figure
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # Plot waveform
    axes[0, 0].plot(np.arange(len(audio)) / sr, audio)
    axes[0, 0].set_title('Audio Waveform')
    axes[0, 0].set_xlabel('Time (s)')
    axes[0, 0].set_ylabel('Amplitude')
    axes[0, 0].grid(True)
    
    # Plot spectrogram
    D = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)
    librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='hz', ax=axes[0, 1])
    axes[0, 1].set_title('Spectrogram')
    
    # Plot MFCC
    mfcc = preprocessor.extract_mfcc_features(audio, sr)
    librosa.display.specshow(mfcc, sr=sr, x_axis='time', ax=axes[1, 0])
    axes[1, 0].set_title('MFCC Features')
    axes[1, 0].set_ylabel('MFCC Coefficients')
    
    # Plot Mel-spectrogram
    mel_spec = preprocessor.extract_mel_spectrogram(audio, sr)
    librosa.display.specshow(mel_spec, sr=sr, x_axis='time', y_axis='mel', ax=axes[1, 1])
    axes[1, 1].set_title('Mel-spectrogram')
    
    plt.tight_layout()
    plt.show()

def create_dataset_statistics(data, labels, class_names):
    """Create and display dataset statistics"""
    
    print("=== Dataset Statistics ===")
    print(f"Total samples: {len(data)}")
    print(f"Feature shape: {data[0].shape}")
    
    # Class distribution
    unique, counts = np.unique(labels, return_counts=True)
    
    plt.figure(figsize=(10, 6))
    bars = plt.bar(range(len(counts)), counts, color='skyblue', edgecolor='black')
    plt.xlabel('Classes')
    plt.ylabel('Number of Samples')
    plt.title('Class Distribution')
    plt.xticks(range(len(counts)), [class_names[i] for i in unique], rotation=45)
    
    #
